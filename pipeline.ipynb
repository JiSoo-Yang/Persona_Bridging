{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c314782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22742f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPersonaDiscoveryPipeline:\n",
    "    \"\"\"\n",
    "    LLM ê°„ ëŒ€í™” ìƒì„± â†’ ë¸Œë¦¬ì§• ì¶”ë¡  â†’ í˜ë¥´ì†Œë‚˜ ë°œê²¬ íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str, persona_schema_path: str):\n",
    "        self.openai_api_key = openai_api_key\n",
    "        openai.api_key = openai_api_key\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ìŠ¤í‚¤ë§ˆ ë¡œë“œ\n",
    "        self.persona_schema = self._load_persona_schema(persona_schema_path)\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (JSONì—ì„œ ìë™ ìƒì„±)\n",
    "        self.persona_categories = self._extract_persona_categories()\n",
    "        \n",
    "        # íƒ€ê²Ÿ LLM ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
    "        self.target_llm_models = [\n",
    "            \"deepseek-ai/deepseek-vl2-tiny\",\n",
    "            \"meta-llama/Llama-3.2-1B\",\n",
    "            \"Qwen/Qwen2.5-0.5B\"\n",
    "        ]\n",
    "        \n",
    "        # ë¡œë“œëœ ëª¨ë¸ ìºì‹œ\n",
    "        self.loaded_models = {}\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ê·¸ë˜í”„\n",
    "        self.persona_graph = nx.DiGraph()\n",
    "        \n",
    "        # ë¸Œë¦¬ì§• ê´€ê³„ ì €ì¥\n",
    "        self.bridging_relations = []\n",
    "        \n",
    "        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "        print(f\"ğŸ“‹ ë¡œë“œëœ í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(self.persona_categories)}\")\n",
    "    \n",
    "    def _load_persona_schema(self, schema_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"í˜ë¥´ì†Œë‚˜ ìŠ¤í‚¤ë§ˆ JSON íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "                schema = json.load(f)\n",
    "            print(f\"âœ… í˜ë¥´ì†Œë‚˜ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {schema_path}\")\n",
    "            return schema\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {schema_path}\")\n",
    "            return self._get_default_schema()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "            return self._get_default_schema()\n",
    "    \n",
    "    def _get_default_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ë°˜í™˜ (íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)\"\"\"\n",
    "        return {\n",
    "            \"structure\": {\n",
    "                \"social_role\": {\n",
    "                    \"categories\": {\n",
    "                        \"professional\": {\"examples\": [\"Doctor\", \"Lawyer\"]},\n",
    "                        \"technical_management\": {\"examples\": [\"Software Engineer\"]},\n",
    "                        \"general_role\": {\"examples\": [\"Student\", \"Artist\"]}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _extract_persona_categories(self) -> List[str]:\n",
    "        \"\"\"JSON ìŠ¤í‚¤ë§ˆì—ì„œ ëª¨ë“  í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ\"\"\"\n",
    "        categories = []\n",
    "        \n",
    "        try:\n",
    "            structure = self.persona_schema.get(\"structure\", {})\n",
    "            social_role = structure.get(\"social_role\", {})\n",
    "            role_categories = social_role.get(\"categories\", {})\n",
    "            \n",
    "            # ëª¨ë“  ì—­í•  ì¹´í…Œê³ ë¦¬ì—ì„œ ì˜ˆì‹œ ì¶”ì¶œ\n",
    "            for category_key, category_data in role_categories.items():\n",
    "                examples = category_data.get(\"examples\", [])\n",
    "                categories.extend(examples)\n",
    "            \n",
    "            print(f\"ğŸ“Š ì¶”ì¶œëœ í˜ë¥´ì†Œë‚˜: {len(categories)}ê°œ\")\n",
    "            \n",
    "            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬\n",
    "            categories = sorted(list(set(categories)))\n",
    "            \n",
    "            return categories\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  í˜ë¥´ì†Œë‚˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ë°˜í™˜\n",
    "            return [\n",
    "                \"Doctor\", \"Lawyer\", \"Teacher\", \"Engineer\", \n",
    "                \"Artist\", \"Writer\", \"Student\", \"Chef\"\n",
    "            ]\n",
    "    \n",
    "    def get_persona_by_category(self, category_type: str) -> List[str]:\n",
    "        \"\"\"íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ í˜ë¥´ì†Œë‚˜ë§Œ ë°˜í™˜\n",
    "        \n",
    "        Args:\n",
    "            category_type: 'professional', 'technical_management', 'general_role'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            structure = self.persona_schema.get(\"structure\", {})\n",
    "            social_role = structure.get(\"social_role\", {})\n",
    "            role_categories = social_role.get(\"categories\", {})\n",
    "            \n",
    "            category_data = role_categories.get(category_type, {})\n",
    "            return category_data.get(\"examples\", [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_all_persona_attributes(self) -> Dict[str, Any]:\n",
    "        \"\"\"í˜ë¥´ì†Œë‚˜ì˜ ëª¨ë“  ì†ì„± ì •ë³´ ë°˜í™˜ (ì„±ê²©, ë°°ê²½, ê´€ì‹¬ì‚¬ ë“±)\"\"\"\n",
    "        return self.persona_schema.get(\"structure\", {})\n",
    "    \n",
    "    def print_persona_summary(self):\n",
    "        \"\"\"ë¡œë“œëœ í˜ë¥´ì†Œë‚˜ ì •ë³´ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ í˜ë¥´ì†Œë‚˜ ìŠ¤í‚¤ë§ˆ ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        structure = self.persona_schema.get(\"structure\", {})\n",
    "        \n",
    "        # 1. ì§ì—…/ì—­í• \n",
    "        social_role = structure.get(\"social_role\", {})\n",
    "        print(\"\\n1ï¸âƒ£  ì§ì—… ë° ì‚¬íšŒì  ì—­í• \")\n",
    "        for cat_key, cat_data in social_role.get(\"categories\", {}).items():\n",
    "            print(f\"   â€¢ {cat_data.get('name', cat_key)}: {len(cat_data.get('examples', []))}ê°œ\")\n",
    "        \n",
    "        # 2. ì„±ê²©\n",
    "        personality = structure.get(\"personality\", {})\n",
    "        print(f\"\\n2ï¸âƒ£  ì„±ê²© íŠ¹ì„± (Big Five): {len(personality.get('categories', []))}ê°œ ì°¨ì›\")\n",
    "        \n",
    "        # 3. ë°°ê²½\n",
    "        background = structure.get(\"background\", {})\n",
    "        print(f\"\\n3ï¸âƒ£  ë°°ê²½ ì •ë³´: {len(background.get('categories', {}))}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "        \n",
    "        # 4. ê´€ì‹¬ì‚¬\n",
    "        interests = structure.get(\"interests\", {})\n",
    "        print(f\"\\n4ï¸âƒ£  ì„ í˜¸ë„ ë° ê´€ì‹¬ì‚¬: {len(interests.get('categories', {}))}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "        \n",
    "        print(f\"\\nâœ… ì´ í˜ë¥´ì†Œë‚˜ ìˆ˜: {len(self.persona_categories)}ê°œ\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    # ==================== ëª¨ë¸ ë¡œë”© ====================\n",
    "\n",
    "    def load_target_model(self, model_name: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
    "        \"\"\"\n",
    "        í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "        \n",
    "        Args:\n",
    "            model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: \"meta-llama/Llama-3.2-1B\")\n",
    "        \n",
    "        Returns:\n",
    "            (tokenizer, model)\n",
    "        \"\"\"\n",
    "        if model_name in self.loaded_models:\n",
    "            print(f\"âœ… ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë“œ: {model_name}\")\n",
    "            return self.loaded_models[model_name]\n",
    "        \n",
    "        print(f\"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.device == \"cpu\":\n",
    "                model = model.to(self.device)\n",
    "            \n",
    "            # íŒ¨ë”© í† í° ì„¤ì •\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            self.loaded_models[model_name] = (tokenizer, model)\n",
    "            print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}\")\n",
    "            \n",
    "            return tokenizer, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}\")\n",
    "            print(f\"   ì˜¤ë¥˜: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    # ==================== STEP 1: í˜ë¥´ì†Œë‚˜ ë¶€ì—¬ ====================\n",
    "\n",
    "    def _get_persona_category_type(self, persona: str) -> str:\n",
    "        \"\"\"í˜ë¥´ì†Œë‚˜ê°€ ì†í•œ ì¹´í…Œê³ ë¦¬ íƒ€ì… ì°¾ê¸°\"\"\"\n",
    "        structure = self.persona_schema.get(\"structure\", {})\n",
    "        social_role = structure.get(\"social_role\", {})\n",
    "        role_categories = social_role.get(\"categories\", {})\n",
    "        \n",
    "        for category_key, category_data in role_categories.items():\n",
    "            if persona in category_data.get(\"examples\", []):\n",
    "                return category_key\n",
    "        \n",
    "        return \"general_role\"  # ê¸°ë³¸ê°’\n",
    "\n",
    "    def _generate_persona_attributes(self, persona: str) -> Dict[str, Any]:\n",
    "        \"\"\"í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” ëœë¤ ì†ì„± ìƒì„±\"\"\"\n",
    "        import random\n",
    "        \n",
    "        structure = self.persona_schema.get(\"structure\", {})\n",
    "        \n",
    "        # ì„±ê²© íŠ¹ì„± (Big Five) ëœë¤ ìƒì„±\n",
    "        personality_traits = {}\n",
    "        for trait_dict in structure.get(\"personality\", {}).get(\"categories\", []):\n",
    "            for trait_name, trait_info in trait_dict.items():\n",
    "                personality_traits[trait_name] = random.choice([\"high\", \"low\"])\n",
    "        \n",
    "        # ë°°ê²½ ì •ë³´ ëœë¤ ì„ íƒ\n",
    "        background_categories = structure.get(\"background\", {}).get(\"categories\", {})\n",
    "        background = {}\n",
    "        for bg_key, bg_data in background_categories.items():\n",
    "            examples = bg_data.get(\"examples\", [])\n",
    "            if examples:\n",
    "                background[bg_key] = random.choice(examples)\n",
    "        \n",
    "        # ê´€ì‹¬ì‚¬ ëœë¤ ì„ íƒ (3-5ê°œ)\n",
    "        interests_categories = structure.get(\"interests\", {}).get(\"categories\", {})\n",
    "        interests = {}\n",
    "        for int_key, int_data in interests_categories.items():\n",
    "            examples = int_data.get(\"examples\", [])\n",
    "            if examples and int_key in [\"hobbies\", \"content_preferences\"]:\n",
    "                # ì·¨ë¯¸ì™€ ì½˜í…ì¸  ì„ í˜¸ë„ëŠ” 2-3ê°œ ì„ íƒ\n",
    "                num_choices = min(random.randint(2, 3), len(examples))\n",
    "                interests[int_key] = random.sample(examples, num_choices)\n",
    "            elif examples:\n",
    "                # ë‚˜ë¨¸ì§€ëŠ” 1ê°œë§Œ ì„ íƒ\n",
    "                interests[int_key] = random.choice(examples)\n",
    "        \n",
    "        return {\n",
    "            \"personality\": personality_traits,\n",
    "            \"background\": background,\n",
    "            \"interests\": interests\n",
    "        }\n",
    "\n",
    "    def assign_persona_to_target_llm(self, target_model: str, persona: str, \n",
    "                                    use_rich_persona: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        íƒ€ê²Ÿ LLMì— í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
    "        \n",
    "        Args:\n",
    "            target_model: íƒ€ê²Ÿ ëª¨ë¸ ì´ë¦„\n",
    "            persona: ë¶€ì—¬í•  í˜ë¥´ì†Œë‚˜ (ì˜ˆ: \"Software Engineer\")\n",
    "            use_rich_persona: í’ë¶€í•œ í˜ë¥´ì†Œë‚˜ ì†ì„± ì‚¬ìš© ì—¬ë¶€\n",
    "        \n",
    "        Returns:\n",
    "            í˜ë¥´ì†Œë‚˜ê°€ ì£¼ì…ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ë©”íƒ€ë°ì´í„°\n",
    "        \"\"\"\n",
    "        # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ íƒ€ì… í™•ì¸\n",
    "        category_type = self._get_persona_category_type(persona)\n",
    "        \n",
    "        if use_rich_persona:\n",
    "            # í’ë¶€í•œ ì†ì„± ìƒì„±\n",
    "            attributes = self._generate_persona_attributes(persona)\n",
    "            \n",
    "            # ì„±ê²© íŠ¹ì„± ì„¤ëª… ìƒì„±\n",
    "            personality_desc = []\n",
    "            personality_map = {\n",
    "                \"openness\": (\"creative and open to new experiences\", \"practical and conventional\"),\n",
    "                \"conscientiousness\": (\"organized and goal-oriented\", \"flexible and spontaneous\"),\n",
    "                \"extroversion\": (\"outgoing and energetic\", \"reserved and thoughtful\"),\n",
    "                \"agreeableness\": (\"cooperative and empathetic\", \"analytical and direct\"),\n",
    "                \"neuroticism\": (\"sensitive and emotional\", \"calm and resilient\")\n",
    "            }\n",
    "            \n",
    "            for trait, level in attributes[\"personality\"].items():\n",
    "                if trait in personality_map:\n",
    "                    desc = personality_map[trait][0] if level == \"high\" else personality_map[trait][1]\n",
    "                    personality_desc.append(desc)\n",
    "            \n",
    "            # ë°°ê²½ ì •ë³´ ë¬¸ìì—´ ìƒì„±\n",
    "            background_parts = []\n",
    "            if \"education\" in attributes[\"background\"]:\n",
    "                background_parts.append(f\"education level: {attributes['background']['education']}\")\n",
    "            if \"age_group\" in attributes[\"background\"]:\n",
    "                background_parts.append(f\"age: {attributes['background']['age_group']}\")\n",
    "            \n",
    "            background_str = \", \".join(background_parts) if background_parts else \"varied background\"\n",
    "            \n",
    "            # ê´€ì‹¬ì‚¬ ë¬¸ìì—´ ìƒì„±\n",
    "            hobbies_str = \"\"\n",
    "            if \"hobbies\" in attributes[\"interests\"]:\n",
    "                hobbies_str = \", \".join(attributes[\"interests\"][\"hobbies\"][:3])\n",
    "            \n",
    "            system_prompt = f\"\"\"You are a {persona} with the following characteristics:\n",
    "\n",
    "    Personality: You are {', '.join(personality_desc[:3])}.\n",
    "    Background: {background_str}\n",
    "    Interests: You enjoy {hobbies_str if hobbies_str else 'various activities related to your profession'}.\n",
    "\n",
    "    When conversing:\n",
    "    - Do NOT explicitly state \"I am a {persona}\" or reveal your role directly.\n",
    "    - Naturally express your interests, experiences, and perspectives through conversation.\n",
    "    - Let your personality traits influence your communication style.\n",
    "    - Reference your background and interests when relevant to the topic.\n",
    "    - Keep responses conversational and authentic (1-3 sentences).\n",
    "    - Use casual, natural language appropriate to your character.\n",
    "    \"\"\"\n",
    "            \n",
    "            korean_prompt = f\"\"\"ë‹¹ì‹ ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì„±ì„ ê°€ì§„ {persona}ì…ë‹ˆë‹¤:\n",
    "\n",
    "    ì„±ê²©: {', '.join(personality_desc[:2])}\n",
    "    ë°°ê²½: {background_str}\n",
    "    ê´€ì‹¬ì‚¬: {hobbies_str if hobbies_str else 'ì§ì—…ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ í™œë™'}\n",
    "\n",
    "    ëŒ€í™”í•  ë•Œ:\n",
    "    - \"ë‚˜ëŠ” {persona}ì…ë‹ˆë‹¤\"ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ë°íˆì§€ ë§ˆì„¸ìš”.\n",
    "    - ëŒ€í™”ë¥¼ í†µí•´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬, ê²½í—˜, ê´€ì ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
    "    - ì„±ê²© íŠ¹ì„±ì´ ëŒ€í™” ìŠ¤íƒ€ì¼ì— ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜ë˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "    - ì£¼ì œì™€ ê´€ë ¨ì´ ìˆì„ ë•Œ ë°°ê²½ê³¼ ê´€ì‹¬ì‚¬ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.\n",
    "    - ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¥¼ ìœ ì§€í•˜ì„¸ìš” (1-3ë¬¸ì¥).\n",
    "    \"\"\"\n",
    "        else:\n",
    "            # ê°„ë‹¨í•œ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸\n",
    "            system_prompt = f\"\"\"You are a {persona}.\n",
    "    When conversing, follow these guidelines:\n",
    "    - Do not explicitly reveal your identity as a {persona}.\n",
    "    - Naturally express interests, habits, and experiences that a {persona} would have.\n",
    "    - Keep responses short and conversational (1-3 sentences).\n",
    "    - Use casual, natural language.\n",
    "    \"\"\"\n",
    "            \n",
    "            korean_prompt = f\"\"\"ë‹¹ì‹ ì€ {persona}ì…ë‹ˆë‹¤.\n",
    "    ëŒ€í™”í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì´ í–‰ë™í•˜ì„¸ìš”:\n",
    "    - ìì‹ ì˜ ì •ì²´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ì§€ ë§ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.\n",
    "    - {persona}ê°€ ê°€ì§ˆ ë²•í•œ ê´€ì‹¬ì‚¬, ìŠµê´€, ê²½í—˜ì„ ì•”ë¬µì ìœ¼ë¡œ ë“œëŸ¬ë‚´ì„¸ìš”.\n",
    "    - ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¥¼ ìœ ì§€í•˜ì„¸ìš” (1-3ë¬¸ì¥).\n",
    "    \"\"\"\n",
    "            attributes = None\n",
    "        \n",
    "        return {\n",
    "            \"model\": target_model,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"system_prompt_ko\": korean_prompt,\n",
    "            \"assigned_persona\": persona,\n",
    "            \"category_type\": category_type,\n",
    "            \"attributes\": attributes,\n",
    "            \"use_rich_persona\": use_rich_persona\n",
    "        }\n",
    "\n",
    "    # ==================== STEP 2: ëŒ€í™” ìƒì„± ====================\n",
    "\n",
    "    def generate_conversation_prompt_for_tool_llm(self, \n",
    "                                                persona_info: Dict = None,\n",
    "                                                turn_count: int = 6) -> str:\n",
    "        \"\"\"\n",
    "        ë„êµ¬ LLM(GPT-4)ì—ê²Œ ì¤„ ëŒ€í™” ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "        \n",
    "        Args:\n",
    "            persona_info: í˜ë¥´ì†Œë‚˜ ì •ë³´ (ì˜µì…˜)\n",
    "            turn_count: ìƒì„±í•  ëŒ€í™” í„´ ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´\n",
    "        \"\"\"\n",
    "        base_prompt = f\"\"\"You are a curious interviewer conducting a natural, engaging conversation.\n",
    "\n",
    "    Generate questions following these rules for {turn_count} turns:\n",
    "\n",
    "    1. Ask naturally about daily life, hobbies, work experiences, and interests.\n",
    "    2. Use open-ended questions to encourage detailed responses.\n",
    "    3. Build on previous answers with thoughtful follow-up questions.\n",
    "    4. Keep questions concise and conversational (1-2 sentences).\n",
    "    5. Avoid yes/no questions when possible.\n",
    "    \"\"\"\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ì¶¤í˜• ì§ˆë¬¸ ê°€ì´ë“œ ì¶”ê°€\n",
    "        if persona_info and persona_info.get(\"attributes\"):\n",
    "            attributes = persona_info[\"attributes\"]\n",
    "            interests = attributes.get(\"interests\", {})\n",
    "            \n",
    "            interest_topics = []\n",
    "            if \"hobbies\" in interests:\n",
    "                interest_topics.extend(interests[\"hobbies\"][:2])\n",
    "            if \"content_preferences\" in interests:\n",
    "                interest_topics.extend(interests[\"content_preferences\"][:2])\n",
    "            \n",
    "            if interest_topics:\n",
    "                base_prompt += f\"\"\"\n",
    "    Consider exploring topics related to: {', '.join(interest_topics)}.\n",
    "    \"\"\"\n",
    "        \n",
    "        base_prompt += \"\"\"\n",
    "    Example questions:\n",
    "    - \"What did you do over the weekend?\"\n",
    "    - \"What kind of projects are you working on these days?\"\n",
    "    - \"How do you usually spend your free time?\"\n",
    "    - \"What got you interested in that?\"\n",
    "\n",
    "    Now, start the conversation with your first question.\n",
    "    \"\"\"\n",
    "        \n",
    "        return base_prompt\n",
    "\n",
    "    def conduct_conversation(\n",
    "        self, \n",
    "        target_config: Dict, \n",
    "        turn_count: int = 6,\n",
    "        language: str = \"en\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ë„êµ¬ LLMê³¼ íƒ€ê²Ÿ LLM ê°„ ëŒ€í™” ì§„í–‰\n",
    "        \n",
    "        Args:\n",
    "            target_config: íƒ€ê²Ÿ LLM ì„¤ì • (assign_persona_to_target_llm ë°˜í™˜ê°’)\n",
    "            turn_count: ëŒ€í™” í„´ ìˆ˜\n",
    "            language: ëŒ€í™” ì–¸ì–´ (\"en\" or \"ko\")\n",
    "        \n",
    "        Returns:\n",
    "            ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
    "        \"\"\"\n",
    "        conversation_history = []\n",
    "        \n",
    "        # íƒ€ê²Ÿ ëª¨ë¸ ë¡œë“œ\n",
    "        tokenizer, model = self.load_target_model(target_config['model'])\n",
    "        \n",
    "        if tokenizer is None or model is None:\n",
    "            print(\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëŒ€í™” ìƒì„± ì¤‘ë‹¨\")\n",
    "            return []\n",
    "        \n",
    "        # ë„êµ¬ LLM ì´ˆê¸° í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ ì •ë³´ í¬í•¨)\n",
    "        tool_llm_prompt = self.generate_conversation_prompt_for_tool_llm(\n",
    "            persona_info=target_config,  # âœ… ì¶”ê°€ë¨\n",
    "            turn_count=turn_count\n",
    "        )\n",
    "        \n",
    "        for turn in range(turn_count):\n",
    "            # 1. ë„êµ¬ LLMì´ ì§ˆë¬¸ ìƒì„±\n",
    "            if turn == 0:\n",
    "                tool_llm_input = tool_llm_prompt\n",
    "            else:\n",
    "                # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨\n",
    "                context = \"\\n\".join([\n",
    "                    f\"{msg['speaker']}: {msg['text']}\" \n",
    "                    for msg in conversation_history[-4:]\n",
    "                ])\n",
    "                tool_llm_input = f\"Previous conversation:\\n{context}\\n\\nAsk the next question.\"\n",
    "            \n",
    "            tool_llm_response = self._call_tool_llm(tool_llm_input)  # âœ… í•¨ìˆ˜ëª… í†µì¼\n",
    "            conversation_history.append({\n",
    "                \"speaker\": \"tool_llm\",\n",
    "                \"text\": tool_llm_response\n",
    "            })\n",
    "            \n",
    "            print(f\"[Tool LLM] {tool_llm_response}\")\n",
    "            \n",
    "            # 2. íƒ€ê²Ÿ LLMì´ ë‹µë³€ ìƒì„±\n",
    "            target_llm_response = self._call_target_llm_huggingface(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                target_config=target_config,\n",
    "                user_input=tool_llm_response,\n",
    "                conversation_history=conversation_history\n",
    "            )\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"speaker\": \"target_llm\",\n",
    "                \"text\": target_llm_response\n",
    "            })\n",
    "            \n",
    "            print(f\"[Target LLM] {target_llm_response}\\n\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return conversation_history\n",
    "\n",
    "    def _call_tool_llm(self, prompt: str, system_prompt: str = None) -> str:\n",
    "        \"\"\"\n",
    "        ë„êµ¬ LLM (GPT-4) API í˜¸ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸\n",
    "            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì˜µì…˜)\n",
    "        \n",
    "        Returns:\n",
    "            GPT-4 ì‘ë‹µ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client = OpenAI(api_key=self.openai_api_key)\n",
    "            \n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ëª…\n",
    "                messages=messages,\n",
    "                max_tokens=150,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPT-4 í˜¸ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "            return \"[Error occurred]\"\n",
    "\n",
    "    def _call_target_llm_huggingface(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        model: AutoModelForCausalLM,\n",
    "        target_config: Dict,\n",
    "        user_input: str,\n",
    "        conversation_history: List[Dict]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ í˜¸ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: ëª¨ë¸ í† í¬ë‚˜ì´ì €\n",
    "            model: ë¡œë“œëœ ëª¨ë¸\n",
    "            target_config: íƒ€ê²Ÿ ì„¤ì •\n",
    "            user_input: ì‚¬ìš©ì ì…ë ¥\n",
    "            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
    "        \n",
    "        Returns:\n",
    "            ëª¨ë¸ ì‘ë‹µ\n",
    "        \"\"\"\n",
    "        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "        system_prompt = target_config['system_prompt']\n",
    "        \n",
    "        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ìµœê·¼ 4í„´)\n",
    "        context = \"\"\n",
    "        for msg in conversation_history[-4:]:\n",
    "            if msg['speaker'] == 'tool_llm':\n",
    "                context += f\"Human: {msg['text']}\\n\"\n",
    "            else:\n",
    "                context += f\"Assistant: {msg['text']}\\n\"\n",
    "        \n",
    "        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        full_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "    {context}Human: {user_input}\n",
    "    Assistant:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # í† í¬ë‚˜ì´ì§•\n",
    "            inputs = tokenizer(\n",
    "                full_prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # ìƒì„±\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # ë””ì½”ë”©\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # í”„ë¡¬í”„íŠ¸ ì œê±° í›„ ë‹µë³€ë§Œ ì¶”ì¶œ\n",
    "            if \"Assistant:\" in response:\n",
    "                response = response.split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            # ë„ˆë¬´ ê¸´ ê²½ìš° ì²« 2-3 ë¬¸ì¥ë§Œ ì‚¬ìš©\n",
    "            sentences = response.split('.')\n",
    "            if len(sentences) > 3:\n",
    "                response = '.'.join(sentences[:3]) + '.'\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íƒ€ê²Ÿ LLM ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "            return \"[Error occurred]\"\n",
    "\n",
    "    # ==================== STEP 3: ë¸Œë¦¬ì§• ì¶”ë¡  ====================\n",
    "\n",
    "    def extract_bridging_relations(\n",
    "        self, \n",
    "        conversation: List[Dict]\n",
    "    ) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        ëŒ€í™”ì—ì„œ ë¸Œë¦¬ì§• ê´€ê³„ ì¶”ì¶œ (GPT-4 ê¸°ë°˜)\n",
    "        \n",
    "        Args:\n",
    "            conversation: ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
    "        \n",
    "        Returns:\n",
    "            [(anchor, anaphor, relation_type), ...]\n",
    "        \"\"\"\n",
    "        # íƒ€ê²Ÿ LLMì˜ ë°œí™”ë§Œ ì¶”ì¶œ\n",
    "        target_utterances = [\n",
    "            turn['text'] for turn in conversation \n",
    "            if turn['speaker'] == 'target_llm'\n",
    "        ]\n",
    "        \n",
    "        if not target_utterances:\n",
    "            return []\n",
    "        \n",
    "        # GPT-4ì—ê²Œ ë¸Œë¦¬ì§• ê´€ê³„ ì¶”ì¶œ ìš”ì²­\n",
    "        prompt = self._create_bridging_extraction_prompt(target_utterances)\n",
    "        \n",
    "        bridging_json = self._call_tool_llm_for_bridging(prompt)\n",
    "        \n",
    "        # JSON íŒŒì‹±\n",
    "        try:\n",
    "            # JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "            if \"```json\" in bridging_json:\n",
    "                bridging_json = bridging_json.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in bridging_json:\n",
    "                bridging_json = bridging_json.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            bridging_data = json.loads(bridging_json)\n",
    "            relations = [\n",
    "                (rel['anchor'], rel['anaphor'], rel['relation_type'])\n",
    "                for rel in bridging_data.get('bridging_relations', [])\n",
    "            ]\n",
    "            return relations\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¸Œë¦¬ì§• ê´€ê³„ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            print(f\"   ì‘ë‹µ: {bridging_json[:200]}\")\n",
    "            return []\n",
    "\n",
    "    def _create_bridging_extraction_prompt(self, utterances: List[str]) -> str:\n",
    "        \"\"\"ë¸Œë¦¬ì§• ê´€ê³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„  ë²„ì „)\"\"\"\n",
    "        utterances_text = \"\\n\".join([f\"{i+1}. {utt}\" for i, utt in enumerate(utterances)])\n",
    "        \n",
    "        prompt = f\"\"\"Extract **Bridging Inference** relations from the following utterances.\n",
    "\n",
    "    Utterances:\n",
    "    {utterances_text}\n",
    "\n",
    "    Bridging Inference Definition:\n",
    "    - Implicit connections between concepts that require world knowledge to understand\n",
    "    - Resolving references that depend on conceptual relationships rather than direct mention\n",
    "    - Examples require understanding semantic frames, part-whole structures, or causal chains\n",
    "\n",
    "    Relation Types and Examples:\n",
    "\n",
    "    1. **Mereological (ë¶€ë¶„-ì „ì²´ ê´€ê³„)**\n",
    "    - part-of (ë¶€ë¶„): A physical or abstract part of a larger whole\n",
    "        Example: \"I looked into the room. The ceiling was very high.\"\n",
    "        â†’ room (ì•µì»¤) â†’ ceiling (ëŒ€ì‘ì–´) [ì²œì¥ì€ ë°©ì˜ ì¼ë¶€ë¶„]\n",
    "    \n",
    "    - member-of (êµ¬ì„±ì›): A member or element of a collection/group/set\n",
    "        Example: \"The class was diverse. One student stood out.\"\n",
    "        â†’ set/element (\"class/student\") [í•™ìƒì€ ì§‘í•©/ìš”ì†Œ ê´€ê³„]\n",
    "\n",
    "    2. **Frame-related (í”„ë ˆì„ ê´€ë ¨)**\n",
    "    - instrument (ë„êµ¬): A tool or instrument used within an action frame\n",
    "        Example: \"John was murdered yesterday. The knife lay nearby.\"\n",
    "        â†’ murder (ì‚´ì¸ ì‚¬ê±´) â†’ knife (ì¹¼) [ì‚´ì¸ì´ë¼ëŠ” ì‚¬ê±´ì˜ ë„êµ¬ë¡œì„œ ì¹¼]\n",
    "    \n",
    "    - theme (ì£¼ì œ): A central theme or topic within a conceptual frame\n",
    "        Example: \"The conference focused on AI ethics. Privacy concerns were raised.\"\n",
    "        â†’ event (ì‚¬ê±´) â†’ argument (ë…¼ì ) [ì‚¬ê±´/ì‚¬ê±´ì˜ ë…¼ì ìœ¼ë¡œ í”„ë ˆì„í™”]\n",
    "\n",
    "    - cause-of (ì¸ê³¼): Causal relationship between events or states\n",
    "        Example: \"The storm was severe. Many buildings collapsed.\"\n",
    "        â†’ eventuality (ì‚¬ê±´/ìƒí™©) â†’ cause (ì›ì¸) [ì‚¬ê±´ê³¼ ê·¸ ì‚¬ê±´ì˜ ì›ì¸ ê´€ê³„]\n",
    "    \n",
    "    - in (ê³µê°„): Spatial containment or location relationship\n",
    "        Example: \"We visited Paris. The museum was crowded.\"\n",
    "        â†’ eventuality (ì‚¬ê±´/ìƒí™©) â†’ location (ì¥ì†Œ) [ì‚¬ê±´ì´ ë°œìƒí•œ ì¥ì†Œì™€ì˜ ê´€ê³„]\n",
    "    \n",
    "    - temporal (ì‹œê°„): Temporal relationship between events\n",
    "        Example: \"The meeting started. Five minutes later, the CEO arrived.\"\n",
    "        â†’ eventuality (ì‚¬ê±´/ìƒí™©) â†’ time (ì‹œê°„) [ì‚¬ê±´ ë°œìƒì˜ ì‹œê°„ê³¼ì˜ ê´€ê³„]\n",
    "\n",
    "    Important Notes:\n",
    "    - Focus on **implicit** relations that require inference\n",
    "    - The anaphor (ëŒ€ì‘ì–´) should NOT be directly mentioned in the same sentence as anchor (ì•µì»¤)\n",
    "    - Relations should reflect real-world knowledge or conceptual frames\n",
    "    - Avoid marking simple co-reference (e.g., \"the car... it\" is NOT bridging)\n",
    "\n",
    "    Output Requirements:\n",
    "    Return ONLY valid JSON in this exact format:\n",
    "    {{\n",
    "    \"bridging_relations\": [\n",
    "        {{\n",
    "        \"anchor\": \"room\",\n",
    "        \"anaphor\": \"ceiling\", \n",
    "        \"relation_type\": \"part-of\",\n",
    "        \"explanation\": \"ì²œì¥ì€ ë°©ì˜ ì¼ë¶€ë¶„ì´ë¼ëŠ” ë°°ê²½ì§€ì‹ í•„ìš”\"\n",
    "        }},\n",
    "        {{\n",
    "        \"anchor\": \"murder\",\n",
    "        \"anaphor\": \"knife\",\n",
    "        \"relation_type\": \"instrument\", \n",
    "        \"explanation\": \"ì‚´ì¸ ì‚¬ê±´ í”„ë ˆì„ì—ì„œ ì¹¼ì€ ë„êµ¬ ì—­í• \"\n",
    "        }}\n",
    "    ]\n",
    "    }}\n",
    "\n",
    "    If no bridging relations are found, return:\n",
    "    {{\n",
    "    \"bridging_relations\": []\n",
    "    }}\n",
    "    \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def _call_tool_llm_for_bridging(self, prompt: str) -> str:\n",
    "        \"\"\"ë¸Œë¦¬ì§• ì¶”ì¶œìš© GPT-4 í˜¸ì¶œ\"\"\"\n",
    "        try:\n",
    "            client = OpenAI(api_key=self.openai_api_key)\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # âœ… ì‹¤ì œ ëª¨ë¸ëª…\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a linguistics expert specializing in bridging inference analysis.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPT-4 ë¸Œë¦¬ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "            return \"{}\"\n",
    "\n",
    "    # ==================== STEP 4: ê·¸ë˜í”„ ìƒì„± ====================\n",
    "\n",
    "    def build_persona_graph(self, relations: List[Tuple[str, str, str]]):\n",
    "        \"\"\"í˜ë¥´ì†Œë‚˜ ê·¸ë˜í”„ êµ¬ì¶•\"\"\"\n",
    "        self.persona_graph.clear()\n",
    "        \n",
    "        for anchor, anaphor, rel_type in relations:\n",
    "            self.persona_graph.add_edge(\n",
    "                anchor.lower(), \n",
    "                anaphor.lower(), \n",
    "                relation=rel_type\n",
    "            )\n",
    "        \n",
    "        self.bridging_relations = relations\n",
    "        print(f\"âœ… ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {len(self.persona_graph.nodes())}ê°œ ë…¸ë“œ, {len(self.persona_graph.edges())}ê°œ ì—£ì§€\")\n",
    "\n",
    "    def get_central_nodes(self, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"ê·¸ë˜í”„ ì¤‘ì‹¬ ë…¸ë“œ ì¶”ì¶œ (PageRank ê¸°ë°˜)\"\"\"\n",
    "        if len(self.persona_graph) == 0:\n",
    "            print(\"âš ï¸  ê·¸ë˜í”„ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            pagerank = nx.pagerank(self.persona_graph)\n",
    "            sorted_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "            return sorted_nodes[:top_k]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ PageRank ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            return []\n",
    "        \n",
    "    # ==================== STEP 5: í˜ë¥´ì†Œë‚˜ ì¶”ë¡  ====================\n",
    "\n",
    "    def infer_persona_from_graph(self) -> str:\n",
    "        \"\"\"\n",
    "        ê·¸ë˜í”„ ì¤‘ì‹¬ ë…¸ë“œë¡œë¶€í„° í˜ë¥´ì†Œë‚˜ ì¶”ë¡  (GPT-4 ì‚¬ìš©)\n",
    "        \n",
    "        Returns:\n",
    "            ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜\n",
    "        \"\"\"\n",
    "        central_nodes = self.get_central_nodes(top_k=5)\n",
    "        \n",
    "        if not central_nodes:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # GPT-4ì—ê²Œ í˜ë¥´ì†Œë‚˜ ì¶”ë¡  ìš”ì²­\n",
    "        nodes_text = \", \".join([node for node, _ in central_nodes])\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ë¬¸ìœ¼ë¡œ ì¶”ì¶œ\n",
    "        persona_list = \"\\n\".join([f\"- {persona}\" for persona in self.persona_categories])\n",
    "        \n",
    "        prompt = f\"\"\"The following are key concepts extracted from a conversation through bridging inference analysis:\n",
    "    {nodes_text}\n",
    "\n",
    "    Based on these concepts and their relationships, select ONE persona category from the following list:\n",
    "    {persona_list}\n",
    "\n",
    "    Answer with only the most appropriate persona name from the list above.\n",
    "    \"\"\"\n",
    "        \n",
    "        inferred_persona = self._call_tool_llm(prompt).strip()\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°\n",
    "        for category in self.persona_categories:\n",
    "            # ì˜ë¬¸ ë¶€ë¶„ë§Œ ì¶”ì¶œí•´ì„œ ë¹„êµ\n",
    "            category_en = category.split('(')[-1].replace(')', '').strip() if '(' in category else category\n",
    "            if category_en.lower() in inferred_persona.lower() or category in inferred_persona:\n",
    "                return category\n",
    "        \n",
    "        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜\n",
    "        return inferred_persona\n",
    "\n",
    "    # ==================== STEP 6: ë¹„êµ ë¶„ì„ (Baseline) ====================\n",
    "\n",
    "    def baseline_llm_direct_inference(self, conversation: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        ë² ì´ìŠ¤ë¼ì¸: ë¸Œë¦¬ì§• ê·¸ë˜í”„ ì—†ì´ LLMì´ ëŒ€í™” ë‚´ìš©ë§Œ ë³´ê³  ì§ì ‘ í˜ë¥´ì†Œë‚˜ ì¶”ë¡ \n",
    "        \n",
    "        Args:\n",
    "            conversation: ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
    "        \n",
    "        Returns:\n",
    "            ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜\n",
    "        \"\"\"\n",
    "        # íƒ€ê²Ÿ LLM ë°œí™”ë§Œ ì¶”ì¶œ\n",
    "        target_utterances = [\n",
    "            turn['text'] for turn in conversation \n",
    "            if turn['speaker'] == 'target_llm'\n",
    "        ]\n",
    "        \n",
    "        if not target_utterances:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # ëŒ€í™” ë‚´ìš© í¬ë§·íŒ…\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{i+1}. {utt}\" for i, utt in enumerate(target_utterances)\n",
    "        ])\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸\n",
    "        persona_list = \"\\n\".join([f\"- {persona}\" for persona in self.persona_categories])\n",
    "        \n",
    "        # GPT-4ì—ê²Œ ì§ì ‘ ì¶”ë¡  ìš”ì²­ (ë¸Œë¦¬ì§• ì •ë³´ ì—†ì´)\n",
    "        prompt = f\"\"\"Read the following conversation responses and infer the speaker's persona.\n",
    "\n",
    "    Conversation responses:\n",
    "    {conversation_text}\n",
    "\n",
    "    Based on the conversation content, select ONE persona category from the following list:\n",
    "    {persona_list}\n",
    "\n",
    "    Answer with only the most appropriate persona name from the list above.\n",
    "    Do not use any external knowledge or bridging inference - only use what is explicitly stated in the conversation.\n",
    "    \"\"\"\n",
    "        \n",
    "        try:\n",
    "            inferred_persona = self._call_tool_llm(prompt).strip()\n",
    "            \n",
    "            # ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°\n",
    "            for category in self.persona_categories:\n",
    "                category_en = category.split('(')[-1].replace(')', '').strip() if '(' in category else category\n",
    "                if category_en.lower() in inferred_persona.lower() or category in inferred_persona:\n",
    "                    return category\n",
    "            \n",
    "            return inferred_persona\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Baseline ì¶”ë¡  ì‹¤íŒ¨: {e}\")\n",
    "            return \"Unknown (Error)\"\n",
    "\n",
    "    # ==================== STEP 7: ì‹œê°í™” & ë¦¬í¬íŠ¸ ====================\n",
    "\n",
    "    def visualize_graph(self, save_path: str = 'persona_graph.png'):\n",
    "        \"\"\"ê·¸ë˜í”„ ì‹œê°í™”\"\"\"\n",
    "        if len(self.persona_graph) == 0:\n",
    "            print(\"âŒ ê·¸ë˜í”„ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(self.persona_graph, k=0.5, seed=42)\n",
    "        \n",
    "        # ë…¸ë“œ í¬ê¸° (PageRank ê¸°ë°˜)\n",
    "        try:\n",
    "            pagerank = nx.pagerank(self.persona_graph)\n",
    "            node_sizes = [pagerank.get(node, 0.01) * 5000 for node in self.persona_graph.nodes()]\n",
    "        except:\n",
    "            node_sizes = [1000] * len(self.persona_graph.nodes())\n",
    "        \n",
    "        nx.draw_networkx_nodes(\n",
    "            self.persona_graph, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color='lightblue',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        nx.draw_networkx_labels(\n",
    "            self.persona_graph, pos,\n",
    "            font_size=10,\n",
    "            font_weight='bold'\n",
    "        )\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            self.persona_graph, pos,\n",
    "            edge_color='gray',\n",
    "            arrows=True,\n",
    "            arrowsize=20,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        # ì—£ì§€ ë¼ë²¨ (ê´€ê³„ íƒ€ì…)\n",
    "        edge_labels = nx.get_edge_attributes(self.persona_graph, 'relation')\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            self.persona_graph, pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_size=8\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Persona Discourse Graph (Bridging Inference)\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"âœ… ê·¸ë˜í”„ ì €ì¥: {save_path}\")\n",
    "\n",
    "    def generate_evaluation_report(\n",
    "        self, \n",
    "        assigned_persona: str,\n",
    "        inferred_persona: str,\n",
    "        baseline_persona: str,\n",
    "        conversation: List[Dict],\n",
    "        model_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        \n",
    "        # ì •í™•ë„ ê³„ì‚° (ì˜ë¬¸ í˜ë¥´ì†Œë‚˜ëª… ê¸°ì¤€)\n",
    "        def extract_persona_name(persona_str):\n",
    "            \"\"\"í˜ë¥´ì†Œë‚˜ ë¬¸ìì—´ì—ì„œ í•µì‹¬ ì´ë¦„ ì¶”ì¶œ\"\"\"\n",
    "            if '(' in persona_str:\n",
    "                # \"ì—¬í–‰ì (Traveler)\" í˜•ì‹ì¸ ê²½ìš° ì˜ë¬¸ ì¶”ì¶œ\n",
    "                return persona_str.split('(')[-1].replace(')', '').strip().lower()\n",
    "            return persona_str.strip().lower()\n",
    "        \n",
    "        assigned_name = extract_persona_name(assigned_persona)\n",
    "        inferred_name = extract_persona_name(inferred_persona)\n",
    "        baseline_name = extract_persona_name(baseline_persona)\n",
    "        \n",
    "        bridging_correct = assigned_name in inferred_name or inferred_name in assigned_name\n",
    "        baseline_correct = assigned_name in baseline_name or baseline_name in assigned_name\n",
    "        \n",
    "        report = f\"\"\"\n",
    "    {'='*70}\n",
    "    LLM Persona Discovery Evaluation Report\n",
    "    {'='*70}\n",
    "\n",
    "    Target Model: {model_name}\n",
    "\n",
    "    1. Ground Truth (ë¶€ì—¬ëœ í˜ë¥´ì†Œë‚˜):\n",
    "    â†’ {assigned_persona}\n",
    "\n",
    "    2. Bridging Inference Method (ì œì•ˆ ë°©ë²• - ê·¸ë˜í”„ ê¸°ë°˜):\n",
    "    â†’ ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜: {inferred_persona}\n",
    "    â†’ ì •í™•ë„: {'âœ… ì¼ì¹˜' if bridging_correct else 'âŒ ë¶ˆì¼ì¹˜'}\n",
    "\n",
    "    3. Baseline (LLM ì§ì ‘ ì¶”ë¡  - ê·¸ë˜í”„ ì—†ì´):\n",
    "    â†’ ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜: {baseline_persona}\n",
    "    â†’ ì •í™•ë„: {'âœ… ì¼ì¹˜' if baseline_correct else 'âŒ ë¶ˆì¼ì¹˜'}\n",
    "\n",
    "    4. Method Comparison:\n",
    "    â€¢ Bridging Method: {'ì •í™•' if bridging_correct else 'ë¶€ì •í™•'}\n",
    "    â€¢ Baseline Method: {'ì •í™•' if baseline_correct else 'ë¶€ì •í™•'}\n",
    "    â€¢ Winner: {'ğŸ† Bridging Method' if bridging_correct and not baseline_correct else 'ğŸ† Baseline Method' if baseline_correct and not bridging_correct else 'ğŸ¤ Both Correct' if bridging_correct and baseline_correct else 'âŒ Both Incorrect'}\n",
    "\n",
    "    5. ë¸Œë¦¬ì§• ê´€ê³„ (ì´ {len(self.bridging_relations)}ê°œ):\n",
    "    \"\"\"\n",
    "        \n",
    "        if self.bridging_relations:\n",
    "            for i, (anchor, anaphor, rel_type) in enumerate(self.bridging_relations[:10], 1):\n",
    "                report += f\"   {i}. {anchor} â†’ {anaphor} [{rel_type}]\\n\"\n",
    "            \n",
    "            if len(self.bridging_relations) > 10:\n",
    "                report += f\"   ... (and {len(self.bridging_relations) - 10} more)\\n\"\n",
    "        else:\n",
    "            report += \"   (ë¸Œë¦¬ì§• ê´€ê³„ ì—†ìŒ)\\n\"\n",
    "        \n",
    "        report += f\"\\n6. ê·¸ë˜í”„ ì¤‘ì‹¬ ë…¸ë“œ (Top 5):\\n\"\n",
    "        central_nodes = self.get_central_nodes(5)\n",
    "        if central_nodes:\n",
    "            for node, score in central_nodes:\n",
    "                report += f\"   â€¢ {node}: {score:.4f}\\n\"\n",
    "        else:\n",
    "            report += \"   (ê·¸ë˜í”„ ë¹„ì–´ìˆìŒ)\\n\"\n",
    "        \n",
    "        report += f\"\\n7. ëŒ€í™” ìƒ˜í”Œ (ì²˜ìŒ 4í„´):\\n\"\n",
    "        sample_turns = conversation[:8]  # 4ê°œ ì§ˆë¬¸-ì‘ë‹µ ìŒ\n",
    "        for i, turn in enumerate(sample_turns):\n",
    "            speaker = \"ğŸ¤– Interviewer\" if turn['speaker'] == 'tool_llm' else \"ğŸ’¬ Target LLM\"\n",
    "            report += f\"   [{speaker}] {turn['text']}\\n\"\n",
    "        \n",
    "        report += f\"\\n{'='*70}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def cleanup_models(self):\n",
    "        \"\"\"ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        for model_name, (tokenizer, model) in self.loaded_models.items():\n",
    "            del tokenizer\n",
    "            del model\n",
    "        self.loaded_models.clear()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"ğŸ§¹ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "    def save_results_to_json(\n",
    "        self,\n",
    "        assigned_persona: str,\n",
    "        inferred_persona: str,\n",
    "        baseline_persona: str,\n",
    "        conversation: List[Dict],\n",
    "        model_name: str,\n",
    "        save_path: str = \"results.json\"\n",
    "    ):\n",
    "        \"\"\"ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        \n",
    "        def extract_persona_name(persona_str):\n",
    "            if '(' in persona_str:\n",
    "                return persona_str.split('(')[-1].replace(')', '').strip()\n",
    "            return persona_str.strip()\n",
    "        \n",
    "        assigned_name = extract_persona_name(assigned_persona)\n",
    "        inferred_name = extract_persona_name(inferred_persona)\n",
    "        baseline_name = extract_persona_name(baseline_persona)\n",
    "        \n",
    "        results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"assigned_persona\": assigned_persona,\n",
    "            \"inferred_persona\": inferred_persona,\n",
    "            \"baseline_persona\": baseline_persona,\n",
    "            \"bridging_correct\": assigned_name.lower() in inferred_name.lower(),\n",
    "            \"baseline_correct\": assigned_name.lower() in baseline_name.lower(),\n",
    "            \"bridging_relations\": [\n",
    "                {\n",
    "                    \"anchor\": anchor,\n",
    "                    \"anaphor\": anaphor,\n",
    "                    \"relation_type\": rel_type\n",
    "                }\n",
    "                for anchor, anaphor, rel_type in self.bridging_relations\n",
    "            ],\n",
    "            \"central_nodes\": [\n",
    "                {\"node\": node, \"score\": float(score)}\n",
    "                for node, score in self.get_central_nodes(10)\n",
    "            ],\n",
    "            \"conversation\": conversation\n",
    "        }\n",
    "        \n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… ê²°ê³¼ ì €ì¥: {save_path}\")\n",
    "\n",
    "    # ==================== ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ====================\n",
    "\n",
    "def run_full_pipeline(\n",
    "    target_model_name: str, \n",
    "    persona: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    turn_count: int = 6,\n",
    "    use_rich_persona: bool = True,\n",
    "    save_json: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    \n",
    "    Args:\n",
    "        target_model_name: íƒ€ê²Ÿ LLM ëª¨ë¸ëª…\n",
    "        persona: ë¶€ì—¬í•  í˜ë¥´ì†Œë‚˜\n",
    "        api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)\n",
    "        turn_count: ëŒ€í™” í„´ ìˆ˜\n",
    "        use_rich_persona: í’ë¶€í•œ í˜ë¥´ì†Œë‚˜ ì†ì„± ì‚¬ìš© ì—¬ë¶€\n",
    "        save_json: JSON ê²°ê³¼ ì €ì¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    \n",
    "    # API í‚¤ ë¡œë“œ\n",
    "    if api_key is None:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"âŒ OPENAI_API_KEY not found in environment variables\")\n",
    "    \n",
    "    persona_path = '/Users/jisu/Desktop/2025Workshop/persona_schema.json'\n",
    "    pipeline = LLMPersonaDiscoveryPipeline(api_key, persona_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ğŸš€ Starting Pipeline: {target_model_name} as '{persona}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
    "    print(\"\\nğŸ“‹ STEP 1: Assigning Persona to Target LLM\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    target_config = pipeline.assign_persona_to_target_llm(\n",
    "        target_model=target_model_name,\n",
    "        persona=persona,\n",
    "        use_rich_persona=use_rich_persona\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Assigned Persona: {persona}\")\n",
    "    if use_rich_persona and target_config.get('attributes'):\n",
    "        print(f\"   Category: {target_config['category_type']}\")\n",
    "        attrs = target_config['attributes']\n",
    "        if 'personality' in attrs:\n",
    "            print(f\"   Personality: {list(attrs['personality'].items())[:2]}\")\n",
    "    \n",
    "    # 2. ëŒ€í™” ìƒì„±\n",
    "    print(\"\\nğŸ’¬ STEP 2: Generating Conversation\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    conversation = pipeline.conduct_conversation(\n",
    "        target_config=target_config,\n",
    "        turn_count=turn_count,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    \n",
    "    if not conversation:\n",
    "        print(\"âŒ ëŒ€í™” ìƒì„± ì‹¤íŒ¨\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ… Generated {len(conversation)} conversation turns\")\n",
    "    \n",
    "    # 3. ë¸Œë¦¬ì§• ê´€ê³„ ì¶”ì¶œ\n",
    "    print(\"\\nğŸ”— STEP 3: Extracting Bridging Relations\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    bridging_relations = pipeline.extract_bridging_relations(conversation)\n",
    "    \n",
    "    if bridging_relations:\n",
    "        print(f\"âœ… Extracted {len(bridging_relations)} bridging relations:\")\n",
    "        for anchor, anaphor, rel_type in bridging_relations[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ\n",
    "            print(f\"   {anchor} â†’ {anaphor} [{rel_type}]\")\n",
    "        if len(bridging_relations) > 5:\n",
    "            print(f\"   ... and {len(bridging_relations) - 5} more\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No bridging relations found\")\n",
    "    \n",
    "    # 4. ê·¸ë˜í”„ êµ¬ì¶•\n",
    "    print(\"\\nğŸ“Š STEP 4: Building Persona Graph\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    pipeline.build_persona_graph(bridging_relations)\n",
    "    \n",
    "    # 5. í˜ë¥´ì†Œë‚˜ ì¶”ë¡  (ì œì•ˆ ë°©ë²•)\n",
    "    print(\"\\nğŸ¯ STEP 5: Inferring Persona (Bridging Method)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    inferred_persona = pipeline.infer_persona_from_graph()\n",
    "    print(f\"âœ… Inferred Persona: {inferred_persona}\")\n",
    "    \n",
    "    # 6. ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ\n",
    "    print(\"\\nğŸ“ˆ STEP 6: Baseline Comparison (LLM Direct Inference)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    baseline_persona = pipeline.baseline_llm_direct_inference(conversation)\n",
    "    print(f\"âœ… Baseline Persona: {baseline_persona}\")\n",
    "    \n",
    "    # 7. í‰ê°€ ë¦¬í¬íŠ¸\n",
    "    print(\"\\nğŸ“„ STEP 7: Generating Evaluation Report\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    report = pipeline.generate_evaluation_report(\n",
    "        assigned_persona=persona,\n",
    "        inferred_persona=inferred_persona,\n",
    "        baseline_persona=baseline_persona,\n",
    "        conversation=conversation,\n",
    "        model_name=target_model_name\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # 8. ì‹œê°í™”\n",
    "    print(\"\\nğŸ¨ STEP 8: Visualizing Graph\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    safe_model_name = target_model_name.replace('/', '_')\n",
    "    safe_persona_name = persona.split('(')[0].strip().replace(' ', '_')\n",
    "    graph_path = f'persona_graph_{safe_model_name}_{safe_persona_name}.png'\n",
    "    \n",
    "    if len(pipeline.persona_graph) > 0:\n",
    "        pipeline.visualize_graph(graph_path)\n",
    "    else:\n",
    "        print(\"âš ï¸  ê·¸ë˜í”„ê°€ ë¹„ì–´ìˆì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤\")\n",
    "    \n",
    "    # 9. ê²°ê³¼ ì €ì¥\n",
    "    result = {\n",
    "        'model': target_model_name,\n",
    "        'assigned_persona': persona,\n",
    "        'inferred_persona': inferred_persona,\n",
    "        'baseline_persona': baseline_persona,\n",
    "        'bridging_correct': _check_persona_match(persona, inferred_persona),\n",
    "        'baseline_correct': _check_persona_match(persona, baseline_persona),\n",
    "        'num_bridging_relations': len(bridging_relations),\n",
    "        'num_graph_nodes': len(pipeline.persona_graph.nodes()),\n",
    "        'num_graph_edges': len(pipeline.persona_graph.edges())\n",
    "    }\n",
    "    \n",
    "    # JSON ì €ì¥\n",
    "    if save_json:\n",
    "        json_path = f'results_{safe_model_name}_{safe_persona_name}.json'\n",
    "        pipeline.save_results_to_json(\n",
    "            assigned_persona=persona,\n",
    "            inferred_persona=inferred_persona,\n",
    "            baseline_persona=baseline_persona,\n",
    "            conversation=conversation,\n",
    "            model_name=target_model_name,\n",
    "            save_path=json_path\n",
    "        )\n",
    "    \n",
    "    # 10. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    print(\"\\nğŸ§¹ STEP 9: Cleaning Up\")\n",
    "    print(\"-\" * 70)\n",
    "    pipeline.cleanup_models()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _check_persona_match(assigned: str, inferred: str) -> bool:\n",
    "    \"\"\"\n",
    "    í˜ë¥´ì†Œë‚˜ ë§¤ì¹­ ì—¬ë¶€ í™•ì¸\n",
    "    \n",
    "    Args:\n",
    "        assigned: ë¶€ì—¬ëœ í˜ë¥´ì†Œë‚˜\n",
    "        inferred: ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜\n",
    "    \n",
    "    Returns:\n",
    "        ë§¤ì¹­ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    def extract_name(persona_str):\n",
    "        if '(' in persona_str:\n",
    "            return persona_str.split('(')[-1].replace(')', '').strip().lower()\n",
    "        return persona_str.strip().lower()\n",
    "    \n",
    "    assigned_name = extract_name(assigned)\n",
    "    inferred_name = extract_name(inferred)\n",
    "    \n",
    "    return assigned_name in inferred_name or inferred_name in assigned_name\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "    models: List[str] = None,\n",
    "    personas: List[str] = None,\n",
    "    api_key: Optional[str] = None,\n",
    "    turn_count: int = 6,\n",
    "    use_rich_persona: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰\n",
    "    \n",
    "    Args:\n",
    "        models: ì‹¤í—˜í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)\n",
    "        personas: ì‹¤í—˜í•  í˜ë¥´ì†Œë‚˜ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)\n",
    "        api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)\n",
    "        turn_count: ëŒ€í™” í„´ ìˆ˜\n",
    "        use_rich_persona: í’ë¶€í•œ í˜ë¥´ì†Œë‚˜ ì‚¬ìš© ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    # API í‚¤ ë¡œë“œ\n",
    "    if api_key is None:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"âŒ OPENAI_API_KEY not found in environment variables\")\n",
    "    \n",
    "    # ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
    "    if models is None:\n",
    "        models = [\n",
    "            \"meta-llama/Llama-3.2-1B\",\n",
    "            \"Qwen/Qwen2.5-0.5B\",\n",
    "            # \"deepseek-ai/deepseek-vl2-tiny\"  # VL ëª¨ë¸ì€ ì œì™¸\n",
    "        ]\n",
    "    \n",
    "    # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ë¦¬ìŠ¤íŠ¸ (JSONì—ì„œ ì¼ë¶€ ì¶”ì¶œ)\n",
    "    if personas is None:\n",
    "        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”í•´ì„œ í˜ë¥´ì†Œë‚˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        temp_pipeline = LLMPersonaDiscoveryPipeline(api_key)\n",
    "        all_personas = temp_pipeline.persona_categories\n",
    "        \n",
    "        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ê· í˜•ìˆê²Œ ì„ íƒ\n",
    "        professional = temp_pipeline.get_persona_by_category(\"professional\")\n",
    "        technical = temp_pipeline.get_persona_by_category(\"technical_management\")\n",
    "        general = temp_pipeline.get_persona_by_category(\"general_role\")\n",
    "        \n",
    "        personas = []\n",
    "        if professional:\n",
    "            personas.append(professional[0])  # ì²« ë²ˆì§¸ ì „ë¬¸ì§\n",
    "        if technical:\n",
    "            personas.append(technical[0])  # ì²« ë²ˆì§¸ ê¸°ìˆ ì§\n",
    "        if general:\n",
    "            personas.append(general[0])  # ì²« ë²ˆì§¸ ì¼ë°˜ì§\n",
    "        \n",
    "        # í˜ë¥´ì†Œë‚˜ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€\n",
    "        if len(personas) < 3 and len(all_personas) >= 3:\n",
    "            for p in all_personas:\n",
    "                if p not in personas:\n",
    "                    personas.append(p)\n",
    "                if len(personas) >= 3:\n",
    "                    break\n",
    "        \n",
    "        print(f\"âœ… ì„ íƒëœ í˜ë¥´ì†Œë‚˜: {personas}\")\n",
    "    \n",
    "    results = []\n",
    "    total_experiments = len(models) * len(personas)\n",
    "    current_experiment = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ğŸ§ª Starting {total_experiments} Experiments\")\n",
    "    print(f\"   Models: {len(models)}\")\n",
    "    print(f\"   Personas: {len(personas)}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for model in models:\n",
    "        for persona in personas:\n",
    "            current_experiment += 1\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 70)\n",
    "            print(f\"ğŸ§ª Experiment {current_experiment}/{total_experiments}: {model} Ã— {persona}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            try:\n",
    "                result = run_full_pipeline(\n",
    "                    target_model_name=model,\n",
    "                    persona=persona,\n",
    "                    api_key=api_key,\n",
    "                    turn_count=turn_count,\n",
    "                    use_rich_persona=use_rich_persona,\n",
    "                    save_json=True\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    results.append(result)\n",
    "                    print(f\"âœ… Experiment {current_experiment} completed successfully\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸  Experiment {current_experiment} returned no result\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Experiment {current_experiment} failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # API ì†ë„ ì œí•œ ë°©ì§€\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\n\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ No successful experiments\")\n",
    "        return results\n",
    "    \n",
    "    bridging_correct = sum(r['bridging_correct'] for r in results)\n",
    "    baseline_correct = sum(r['baseline_correct'] for r in results)\n",
    "    total = len(results)\n",
    "    \n",
    "    bridging_accuracy = (bridging_correct / total) * 100\n",
    "    baseline_accuracy = (baseline_correct / total) * 100\n",
    "    improvement = bridging_accuracy - baseline_accuracy\n",
    "    \n",
    "    print(f\"\\nTotal Experiments: {total}\")\n",
    "    print(f\"Successful: {len(results)}\")\n",
    "    print(f\"\\n{'Method':<25} {'Correct':<10} {'Accuracy'}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Bridging Inference':<25} {bridging_correct}/{total:<10} {bridging_accuracy:.1f}%\")\n",
    "    print(f\"{'Baseline (LLM Direct)':<25} {baseline_correct}/{total:<10} {baseline_accuracy:.1f}%\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Improvement':<25} {'':<10} {improvement:+.1f}%\")\n",
    "    \n",
    "    # ëª¨ë¸ë³„ ê²°ê³¼\n",
    "    print(\"\\nğŸ“Š Results by Model:\")\n",
    "    for model in models:\n",
    "        model_results = [r for r in results if r['model'] == model]\n",
    "        if model_results:\n",
    "            bridging_acc = sum(r['bridging_correct'] for r in model_results) / len(model_results) * 100\n",
    "            baseline_acc = sum(r['baseline_correct'] for r in model_results) / len(model_results) * 100\n",
    "            print(f\"  {model}\")\n",
    "            print(f\"    Bridging: {bridging_acc:.1f}% | Baseline: {baseline_acc:.1f}%\")\n",
    "    \n",
    "    # í˜ë¥´ì†Œë‚˜ë³„ ê²°ê³¼\n",
    "    print(\"\\nğŸ“Š Results by Persona:\")\n",
    "    for persona in personas:\n",
    "        persona_results = [r for r in results if r['assigned_persona'] == persona]\n",
    "        if persona_results:\n",
    "            bridging_acc = sum(r['bridging_correct'] for r in persona_results) / len(persona_results) * 100\n",
    "            baseline_acc = sum(r['baseline_correct'] for r in persona_results) / len(persona_results) * 100\n",
    "            print(f\"  {persona}\")\n",
    "            print(f\"    Bridging: {bridging_acc:.1f}% | Baseline: {baseline_acc:.1f}%\")\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
    "    summary_path = \"experiment_summary.json\"\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"total_experiments\": total,\n",
    "            \"bridging_accuracy\": bridging_accuracy,\n",
    "            \"baseline_accuracy\": baseline_accuracy,\n",
    "            \"improvement\": improvement,\n",
    "            \"detailed_results\": results\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Summary saved to: {summary_path}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04129aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key loaded successfully\n",
      "âš ï¸  ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: persona_schema.json\n",
      "ğŸ“Š ì¶”ì¶œëœ í˜ë¥´ì†Œë‚˜: 5ê°œ\n",
      "ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
      "ğŸ“‹ ë¡œë“œëœ í˜ë¥´ì†Œë‚˜ ì¹´í…Œê³ ë¦¬ ìˆ˜: 5\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Starting Pipeline: Qwen/Qwen2.5-0.5B as 'Doctor'\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ STEP 1: Assigning Persona to Target LLM\n",
      "----------------------------------------------------------------------\n",
      "âœ… Assigned Persona: Doctor\n",
      "   Category: professional\n",
      "   Personality: []\n",
      "\n",
      "ğŸ’¬ STEP 2: Generating Conversation\n",
      "----------------------------------------------------------------------\n",
      "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: Qwen/Qwen2.5-0.5B\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: Qwen/Qwen2.5-0.5B\n",
      "[Tool LLM] What does a typical day look like for you, and how do you usually start your mornings?\n",
      "[Target LLM] My typical day is filled with different aspects of my life, and I usually start my mornings with a quick morning routine that helps me get ready for the day. I usually wake up around 6:30am, grab my favorite coffee and breakfast, and then do my morning exercises to get my body moving and energized. I usually have a short lunch and then head out to the park for a run or walk to get some exercise and fresh air.\n",
      "\n",
      "[Tool LLM] That sounds like a great way to start the day! What kind of exercise do you usually do in the mornings, and do you have any favorite workouts or activities that you enjoy?\n",
      "[Target LLM] My favorite exercise is running. I love the feeling of pushing myself to get up and move, and the sound of the wind in my hair. I also enjoy trying new exercises and trying to find new ways to challenge myself.\n",
      "\n",
      "[Tool LLM] Do you have any specific goals or milestones you're working towards with your running or other exercises?\n",
      "[Target LLM] I don't have access to my specific\n",
      "\n",
      "[Tool LLM] What kind of new exercises have you tried recently, and how have they challenged you?\n",
      "[Target LLM] Sure, here are some tips on how to start and build an exercise routine:\n",
      "\n",
      "1. Start small: Begin with a manageable exercise routine and gradually increase the intensity and duration as you build muscle strength and endurance. 2.\n",
      "\n",
      "[Tool LLM] What are some of your favorite exercises or activities to include in your routine, and what do you enjoy most about them?\n",
      "[Target LLM] As an AI language model, I don't have a personal preference or emotional state like humans do. However, I can provide you with some general tips on how to start and build an exercise routine:\n",
      "\n",
      "    1. Set a realistic goal: Decide on an exercise routine that is appropriate for your fitness level and goals.\n",
      "\n",
      "[Tool LLM] What are some of your fitness goals, and how do you plan to achieve them through your exercise routine?\n",
      "[Target LLM] My fitness goals are to build strength, endurance, and flexibility. I plan to start with a high-intensity interval training (HIIT) routine, which involves alternating between high-intensity exercise and rest periods. I also aim to include a variety of exercises, including cardio, strength training, and flexibility work, to help build muscle and improve overall health.\n",
      "\n",
      "âœ… Generated 12 conversation turns\n",
      "\n",
      "ğŸ”— STEP 3: Extracting Bridging Relations\n",
      "----------------------------------------------------------------------\n",
      "âœ… Extracted 11 bridging relations:\n",
      "   morning routine â†’ get ready for the day [frame-role]\n",
      "   wake up â†’ grab my favorite coffee and breakfast [co-occurrence]\n",
      "   morning exercises â†’ get my body moving and energized [cause-of]\n",
      "   lunch â†’ head out to the park [co-occurrence]\n",
      "   run or walk â†’ exercise and fresh air [cause-of]\n",
      "   ... and 6 more\n",
      "\n",
      "ğŸ“Š STEP 4: Building Persona Graph\n",
      "----------------------------------------------------------------------\n",
      "âœ… ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: 22ê°œ ë…¸ë“œ, 11ê°œ ì—£ì§€\n",
      "\n",
      "ğŸ¯ STEP 5: Inferring Persona (Bridging Method)\n",
      "----------------------------------------------------------------------\n",
      "âœ… Inferred Persona: Student\n",
      "\n",
      "ğŸ“ˆ STEP 6: Baseline Comparison (LLM Direct Inference)\n",
      "----------------------------------------------------------------------\n",
      "âœ… Baseline Persona: Software Engineer\n",
      "\n",
      "ğŸ“„ STEP 7: Generating Evaluation Report\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "    ======================================================================\n",
      "    LLM Persona Discovery Evaluation Report\n",
      "    ======================================================================\n",
      "\n",
      "    Target Model: Qwen/Qwen2.5-0.5B\n",
      "\n",
      "    1. Ground Truth (ë¶€ì—¬ëœ í˜ë¥´ì†Œë‚˜):\n",
      "    â†’ Doctor\n",
      "\n",
      "    2. Bridging Inference Method (ì œì•ˆ ë°©ë²• - ê·¸ë˜í”„ ê¸°ë°˜):\n",
      "    â†’ ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜: Student\n",
      "    â†’ ì •í™•ë„: âŒ ë¶ˆì¼ì¹˜\n",
      "\n",
      "    3. Baseline (LLM ì§ì ‘ ì¶”ë¡  - ê·¸ë˜í”„ ì—†ì´):\n",
      "    â†’ ì¶”ë¡ ëœ í˜ë¥´ì†Œë‚˜: Software Engineer\n",
      "    â†’ ì •í™•ë„: âŒ ë¶ˆì¼ì¹˜\n",
      "\n",
      "    4. Method Comparison:\n",
      "    â€¢ Bridging Method: ë¶€ì •í™•\n",
      "    â€¢ Baseline Method: ë¶€ì •í™•\n",
      "    â€¢ Winner: âŒ Both Incorrect\n",
      "\n",
      "    5. ë¸Œë¦¬ì§• ê´€ê³„ (ì´ 11ê°œ):\n",
      "       1. morning routine â†’ get ready for the day [frame-role]\n",
      "   2. wake up â†’ grab my favorite coffee and breakfast [co-occurrence]\n",
      "   3. morning exercises â†’ get my body moving and energized [cause-of]\n",
      "   4. lunch â†’ head out to the park [co-occurrence]\n",
      "   5. run or walk â†’ exercise and fresh air [cause-of]\n",
      "   6. running â†’ pushing myself to get up and move [cause-of]\n",
      "   7. exercise routine â†’ build muscle strength and endurance [cause-of]\n",
      "   8. fitness goals â†’ build strength, endurance, and flexibility [cause-of]\n",
      "   9. HIIT routine â†’ alternating between high-intensity exercise and rest periods [frame-role]\n",
      "   10. variety of exercises â†’ cardio, strength training, and flexibility work [part-of]\n",
      "   ... (and 1 more)\n",
      "\n",
      "6. ê·¸ë˜í”„ ì¤‘ì‹¬ ë…¸ë“œ (Top 5):\n",
      "   â€¢ get ready for the day: 0.0590\n",
      "   â€¢ grab my favorite coffee and breakfast: 0.0590\n",
      "   â€¢ get my body moving and energized: 0.0590\n",
      "   â€¢ head out to the park: 0.0590\n",
      "   â€¢ exercise and fresh air: 0.0590\n",
      "\n",
      "7. ëŒ€í™” ìƒ˜í”Œ (ì²˜ìŒ 4í„´):\n",
      "   [ğŸ¤– Interviewer] What does a typical day look like for you, and how do you usually start your mornings?\n",
      "   [ğŸ’¬ Target LLM] My typical day is filled with different aspects of my life, and I usually start my mornings with a quick morning routine that helps me get ready for the day. I usually wake up around 6:30am, grab my favorite coffee and breakfast, and then do my morning exercises to get my body moving and energized. I usually have a short lunch and then head out to the park for a run or walk to get some exercise and fresh air.\n",
      "   [ğŸ¤– Interviewer] That sounds like a great way to start the day! What kind of exercise do you usually do in the mornings, and do you have any favorite workouts or activities that you enjoy?\n",
      "   [ğŸ’¬ Target LLM] My favorite exercise is running. I love the feeling of pushing myself to get up and move, and the sound of the wind in my hair. I also enjoy trying new exercises and trying to find new ways to challenge myself.\n",
      "   [ğŸ¤– Interviewer] Do you have any specific goals or milestones you're working towards with your running or other exercises?\n",
      "   [ğŸ’¬ Target LLM] I don't have access to my specific\n",
      "   [ğŸ¤– Interviewer] What kind of new exercises have you tried recently, and how have they challenged you?\n",
      "   [ğŸ’¬ Target LLM] Sure, here are some tips on how to start and build an exercise routine:\n",
      "\n",
      "1. Start small: Begin with a manageable exercise routine and gradually increase the intensity and duration as you build muscle strength and endurance. 2.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ğŸ¨ STEP 8: Visualizing Graph\n",
      "----------------------------------------------------------------------\n",
      "âœ… ê·¸ë˜í”„ ì €ì¥: persona_graph_Qwen_Qwen2.5-0.5B_Doctor.png\n",
      "âœ… ê²°ê³¼ ì €ì¥: results_Qwen_Qwen2.5-0.5B_Doctor.json\n",
      "\n",
      "ğŸ§¹ STEP 9: Cleaning Up\n",
      "----------------------------------------------------------------------\n",
      "ğŸ§¹ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "\n",
      "âœ… Single experiment completed: {'model': 'Qwen/Qwen2.5-0.5B', 'assigned_persona': 'Doctor', 'inferred_persona': 'Student', 'baseline_persona': 'Software Engineer', 'bridging_correct': False, 'baseline_correct': False, 'num_bridging_relations': 11, 'num_graph_nodes': 22, 'num_graph_edges': 11}\n"
     ]
    }
   ],
   "source": [
    "# ==================== Jupyter Notebookìš© ì‹¤í–‰ ì½”ë“œ ====================\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âŒ Error: OPENAI_API_KEY environment variable not set\")\n",
    "    print(\"   Please set it in the notebook:\")\n",
    "    print(\"   import os\")\n",
    "    print(\"   os.environ['OPENAI_API_KEY'] = 'your-api-key'\")\n",
    "else:\n",
    "    print(\"âœ… API Key loaded successfully\")\n",
    "\n",
    "# ========== ì˜µì…˜ 1: ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ ==========\n",
    "result = run_full_pipeline(\n",
    "    target_model_name=\"Qwen/Qwen2.5-0.5B\",\n",
    "    persona=\"Doctor\",\n",
    "    api_key=api_key,\n",
    "    turn_count=6,\n",
    "    use_rich_persona=True,\n",
    "    save_json=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Single experiment completed: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81874038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ë©”ì¸ ì‹¤í–‰ ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"LLM Persona Discovery Pipeline\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"Single model to test\")\n",
    "    parser.add_argument(\"--persona\", type=str, help=\"Single persona to test\")\n",
    "    parser.add_argument(\"--turn_count\", type=int, default=6, help=\"Number of conversation turns\")\n",
    "    parser.add_argument(\"--all\", action=\"store_true\", help=\"Run all experiments\")\n",
    "    parser.add_argument(\"--rich_persona\", action=\"store_true\", default=True, help=\"Use rich persona attributes\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ Error: OPENAI_API_KEY environment variable not set\")\n",
    "        print(\"   Please set it using: export OPENAI_API_KEY='your-api-key'\")\n",
    "        exit(1)\n",
    "    \n",
    "    if args.all:\n",
    "        # ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰\n",
    "        results = run_all_experiments(\n",
    "            api_key=api_key,\n",
    "            turn_count=args.turn_count,\n",
    "            use_rich_persona=args.rich_persona\n",
    "        )\n",
    "    elif args.model and args.persona:\n",
    "        # ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰\n",
    "        result = run_full_pipeline(\n",
    "            target_model_name=args.model,\n",
    "            persona=args.persona,\n",
    "            api_key=api_key,\n",
    "            turn_count=args.turn_count,\n",
    "            use_rich_persona=args.rich_persona\n",
    "        )\n",
    "        print(f\"\\nâœ… Single experiment completed: {result}\")\n",
    "    else:\n",
    "        print(\"âŒ Please specify --all or both --model and --persona\")\n",
    "        parser.print_help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99d1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
