{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbf4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Persona Interview System with Hugging Face Models\n",
    "도구 에이전트(GPT)와 타겟 LLM(Hugging Face 모델) 간의 대화 시스템\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Any\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForVision2Seq\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec900b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 유틸리티\n",
    "# ============================================================================\n",
    "\n",
    "def load_json(filepath: str) -> Dict:\n",
    "    \"\"\"JSON 파일 로드\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(data: Dict, filepath: str):\n",
    "    \"\"\"JSON 파일 저장\"\"\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 도구 함수들 (Tool Agent가 사용)\n",
    "# ============================================================================\n",
    "\n",
    "PERSONA_SCHEMA_PATH = \"/Users/jisu/Desktop/2025Workshop/persona_schema.json\"\n",
    "BRIDGING_PATH = \"/Users/jisu/Desktop/2025Workshop/bridging_relationships.json\"\n",
    "\n",
    "\n",
    "def load_persona_definition(dummy: str = \"\") -> str:\n",
    "    \"\"\"페르소나 정의 JSON 파일을 로드합니다.\"\"\"\n",
    "    try:\n",
    "        data = load_json(PERSONA_SCHEMA_PATH)\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error loading persona definition: {str(e)}\"\n",
    "\n",
    "\n",
    "def load_bridging_relationships(dummy: str = \"\") -> str:\n",
    "    \"\"\"브리징 관계 정의 JSON 파일을 로드합니다 (언어학적 정의).\"\"\"\n",
    "    try:\n",
    "        data = load_json(BRIDGING_PATH)\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error loading bridging relationships: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_bridging_from_conversation(conversation: str) -> str:\n",
    "    \"\"\"\n",
    "    대화에서 브리징 관계를 추출합니다.\n",
    "    현재는 정의를 로드해 '가능한 관계 타입 목록'만 수집하는 더미 구현입니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bridging_def = load_json(BRIDGING_PATH)\n",
    "        bridging_types = bridging_def.get(\"bridging_types\", {})\n",
    "\n",
    "        result = {\n",
    "            \"bridging_relationships\": {\n",
    "                \"rules\": [],  # 실제 구현에서는 추출 결과를 rules 리스트에 채웁니다.\n",
    "            },\n",
    "            \"conversation_preview\": conversation[:200] + \"...\" if len(conversation) > 200 else conversation,\n",
    "            \"available_relation_types\": [],\n",
    "            \"note\": \"프로덕션에서는 LLM을 사용해 실제 브리징 관계를 rules에 채우세요.\",\n",
    "        }\n",
    "\n",
    "        for category, data in bridging_types.items():\n",
    "            for relation in data.get(\"relations\", []):\n",
    "                result[\"available_relation_types\"].append(\n",
    "                    {\n",
    "                        \"category\": category,\n",
    "                        \"type\": relation[\"relation_type\"],\n",
    "                        \"korean\": relation[\"korean\"],\n",
    "                        \"definition\": relation[\"definition\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return json.dumps(result, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting bridging: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_bridging_extraction_prompt(utterances: List[str]) -> str:\n",
    "    \"\"\"브리징 관계 추출을 위한 프롬프트 생성 (언어학적 정의 기반)\"\"\"\n",
    "    utterances_text = \"\\n\".join([f\"{i+1}. {utt}\" for i, utt in enumerate(utterances)])\n",
    "    prompt = f\"\"\"Extract **Bridging Inference** relations from the following utterances.\n",
    "\n",
    "Utterances:\n",
    "{utterances_text}\n",
    "\n",
    "Bridging Inference Definition:\n",
    "- Implicit connections between concepts that require world knowledge to understand\n",
    "- Resolving references that depend on conceptual relationships rather than direct mention\n",
    "- Examples require understanding semantic frames, part-whole structures, or causal chains\n",
    "\n",
    "Relation Types and Examples:\n",
    "\n",
    "1. Mereological (부분-전체): part-of, member-of\n",
    "2. Frame-related: instrument, theme, cause-of, in (location), temporal\n",
    "\n",
    "Output JSON:\n",
    "{{\"bridging_relations\": [{{\"anchor\":\"...\", \"anaphor\":\"...\", \"relation_type\":\"...\", \"explanation\":\"...\", \"sentence_context\":\"...\"}}]]}}\n",
    "If none, return {{\"bridging_relations\":[]}}.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_importance_graph(bridging_data: str) -> str:\n",
    "    \"\"\"\n",
    "    브리징 관계를 기반으로 중요도 그래프를 생성합니다.\n",
    "    각 노드의 중요도는 연결된 엣지 수로 결정됩니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(bridging_data)\n",
    "        relationships = data.get(\"bridging_relationships\", {}).get(\"rules\", [])\n",
    "\n",
    "        node_connections = defaultdict(int)\n",
    "        edge_list = []\n",
    "\n",
    "        for rule in relationships:\n",
    "            from_node = rule[\"from\"]\n",
    "            to_nodes = rule[\"to\"]\n",
    "            strength = rule.get(\"strength\", \"medium\")\n",
    "\n",
    "            for to_node in to_nodes:\n",
    "                node_connections[from_node] += 1\n",
    "                node_connections[to_node] += 1\n",
    "                edge_list.append({\"from\": from_node, \"to\": to_node, \"strength\": strength})\n",
    "\n",
    "        max_connections = max(node_connections.values()) if node_connections else 1\n",
    "        importance_scores = {node: count / max_connections for node, count in node_connections.items()}\n",
    "\n",
    "        sorted_nodes = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        graph_result = {\n",
    "            \"nodes\": [{\"name\": node, \"importance\": score, \"connections\": node_connections[node]} for node, score in sorted_nodes],\n",
    "            \"edges\": edge_list,\n",
    "            \"summary\": {\n",
    "                \"total_nodes\": len(node_connections),\n",
    "                \"total_edges\": len(edge_list),\n",
    "                \"most_important\": sorted_nodes[0][0] if sorted_nodes else None,\n",
    "                \"most_important_score\": sorted_nodes[0][1] if sorted_nodes else 0,\n",
    "            },\n",
    "        }\n",
    "        return json.dumps(graph_result, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error creating graph: {str(e)}\"\n",
    "\n",
    "\n",
    "def generate_random_persona(dummy: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    랜덤 페르소나를 생성합니다. (각 카테고리에서 하나 선택)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        persona_def = load_json(PERSONA_SCHEMA_PATH)\n",
    "        structure = persona_def[\"structure\"]\n",
    "\n",
    "        persona = {}\n",
    "\n",
    "        # Social Role\n",
    "        social_categories = structure[\"social_role\"][\"categories\"]\n",
    "        category_key = random.choice(list(social_categories.keys()))\n",
    "        persona[\"social_role\"] = random.choice(social_categories[category_key][\"examples\"])\n",
    "\n",
    "        # Personality (0/1)\n",
    "        personality_traits = {}\n",
    "        for trait_obj in structure[\"personality\"][\"categories\"]:\n",
    "            trait_name = list(trait_obj.keys())[0]\n",
    "            personality_traits[trait_name] = random.choice([\"0\", \"1\"])\n",
    "        persona[\"personality\"] = personality_traits\n",
    "\n",
    "        # Background\n",
    "        background = {}\n",
    "        for bg_key, bg_data in structure[\"background\"][\"categories\"].items():\n",
    "            background[bg_key] = random.choice(bg_data[\"examples\"])\n",
    "        persona[\"background\"] = background\n",
    "\n",
    "        # Interests\n",
    "        interests = {}\n",
    "        for int_key, int_data in structure[\"interests\"][\"categories\"].items():\n",
    "            interests[int_key] = random.choice(int_data[\"examples\"])\n",
    "        persona[\"interests\"] = interests\n",
    "\n",
    "        result = {\"persona\": persona, \"description\": \"Randomly generated persona with one item per category\"}\n",
    "        return json.dumps(result, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        fallback = {\n",
    "            \"error\": f\"{type(e).__name__}: {str(e)}\",\n",
    "            \"persona\": {\n",
    "                \"social_role\": \"student\",\n",
    "                \"personality\": {\"openness\": \"1\", \"conscientiousness\": \"1\", \"extraversion\": \"0\", \"agreeableness\": \"1\", \"neuroticism\": \"0\"},\n",
    "                \"background\": {\"education\": \"unknown\", \"culture\": \"unknown\"},\n",
    "                \"interests\": {\"hobby\": \"reading\"},\n",
    "            },\n",
    "            \"description\": \"Fallback persona due to loading error\",\n",
    "        }\n",
    "        return json.dumps(fallback, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Hugging Face 모델 래퍼 (MPS/CPU/CUDA 안전)\n",
    "# ============================================================================\n",
    "\n",
    "class HuggingFaceModelWrapper:\n",
    "    \"\"\"Hugging Face 모델을 LangChain 스타일로 사용할 수 있게 하는 래퍼\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: HF 모델 이름\n",
    "            device: \"auto\" | \"cuda\" | \"mps\" | \"cpu\"\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = self._resolve_device(device)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Loading model: {model_name}  on device: {self.device}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        trust_code = \"deepseek-vl2\" in model_name.lower()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_code)\n",
    "\n",
    "        prefer_half = self.device in (\"cuda\", \"mps\")\n",
    "        dtype_try = torch.float16 if prefer_half else torch.float32\n",
    "\n",
    "        def _load(dtype):\n",
    "            if \"deepseek-vl2\" in model_name.lower():\n",
    "                return AutoModelForVision2Seq.from_pretrained(model_name, trust_remote_code=True, torch_dtype=dtype)\n",
    "            else:\n",
    "                return AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype)\n",
    "\n",
    "        try:\n",
    "            self.model = _load(dtype_try)\n",
    "            if self.device != \"cpu\":\n",
    "                self.model.to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ dtype {dtype_try} failed on {self.device}: {e}\\n→ Reloading with float32.\")\n",
    "            self.model = _load(torch.float32)\n",
    "            if self.device != \"cpu\":\n",
    "                self.model.to(self.device)\n",
    "\n",
    "        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model.eval()\n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    def _resolve_device(self, device: str) -> str:\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "                return \"mps\"\n",
    "            return \"cpu\"\n",
    "        return device\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.7) -> str:\n",
    "        \"\"\"텍스트 생성\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            if self.device != \"cpu\":\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if prompt in generated_text:\n",
    "                generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating text: {str(e)}\"\n",
    "\n",
    "    def invoke(self, messages: List) -> Any:\n",
    "        \"\"\"LangChain 호환 invoke 메서드\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            if hasattr(msg, \"content\"):\n",
    "                content = msg.content\n",
    "                if isinstance(msg, SystemMessage):\n",
    "                    prompt_parts.append(f\"System: {content}\")\n",
    "                elif isinstance(msg, HumanMessage):\n",
    "                    prompt_parts.append(f\"User: {content}\")\n",
    "\n",
    "        prompt = \"\\n\".join(prompt_parts) + \"\\nAssistant:\"\n",
    "        response_text = self.generate(prompt)\n",
    "\n",
    "        class Response:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "\n",
    "        return Response(response_text)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 타겟 LLM 페르소나 생성\n",
    "# ============================================================================\n",
    "\n",
    "def create_target_persona() -> Dict[str, Any]:\n",
    "    \"\"\"타겟 LLM에 부여할 페르소나 생성 (각 유형별로 한 항목만)\"\"\"\n",
    "    persona_result = json.loads(generate_random_persona())\n",
    "    return persona_result[\"persona\"]\n",
    "\n",
    "\n",
    "def format_persona_for_prompt(persona: Dict[str, Any]) -> str:\n",
    "    \"\"\"페르소나를 프롬프트용 텍스트로 변환\"\"\"\n",
    "    parts = [f\"Occupation/Role: {persona['social_role']}\"]\n",
    "    parts.append(\"\\nPersonality Traits (Big Five):\")\n",
    "    for trait, value in persona[\"personality\"].items():\n",
    "        level = \"High\" if value == \"1\" else \"Low\"\n",
    "        parts.append(f\"  - {trait}: {level}\")\n",
    "\n",
    "    parts.append(\"\\nBackground Information:\")\n",
    "    for key, value in persona[\"background\"].items():\n",
    "        parts.append(f\"  - {key}: {value}\")\n",
    "\n",
    "    parts.append(\"\\nInterests and Preferences:\")\n",
    "    for key, value in persona[\"interests\"].items():\n",
    "        parts.append(f\"  - {key}: {value}\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# @tool 래핑 (LangChain이 자동으로 schema/실행 처리)\n",
    "# ============================================================================\n",
    "\n",
    "@tool\n",
    "def load_persona_definition_tool(dummy: str = \"\") -> str:\n",
    "    \"\"\"Load the persona definition JSON file from disk and return it as a pretty JSON string.\"\"\"\n",
    "    return load_persona_definition(dummy)\n",
    "\n",
    "@tool\n",
    "def load_bridging_relationships_tool(dummy: str = \"\") -> str:\n",
    "    \"\"\"Load linguistic bridging relationship definitions (Mereological / Frame-related) and return as JSON string.\"\"\"\n",
    "    return load_bridging_relationships(dummy)\n",
    "\n",
    "@tool\n",
    "def create_bridging_extraction_prompt_tool(utterances: List[str]) -> str:\n",
    "    \"\"\"Create a prompt for extracting bridging inference relations from the given utterances.\"\"\"\n",
    "    return create_bridging_extraction_prompt(utterances)\n",
    "\n",
    "@tool\n",
    "def extract_bridging_from_conversation_tool(conversation: str) -> str:\n",
    "    \"\"\"Extract bridging relations from a conversation using predefined linguistic categories.\"\"\"\n",
    "    return extract_bridging_from_conversation(conversation)\n",
    "\n",
    "@tool\n",
    "def create_importance_graph_tool(bridging_data: str) -> str:\n",
    "    \"\"\"Build an importance graph (nodes/edges) from bridging relationship JSON data.\"\"\"\n",
    "    return create_importance_graph(bridging_data)\n",
    "\n",
    "@tool\n",
    "def generate_random_persona_tool(dummy: str = \"\") -> str:\n",
    "    \"\"\"Generate a random persona by sampling one item per category from persona schema.\"\"\"\n",
    "    return generate_random_persona(dummy)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 에이전트 생성 (LangChain create_agent)\n",
    "# ============================================================================\n",
    "\n",
    "def create_tool_agent(openai_api_key: str, memory: MemorySaver):\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key, temperature=0.2)\n",
    "\n",
    "    tools = [\n",
    "        load_persona_definition_tool,\n",
    "        load_bridging_relationships_tool,\n",
    "        create_bridging_extraction_prompt_tool,\n",
    "        extract_bridging_from_conversation_tool,\n",
    "        create_importance_graph_tool,\n",
    "        generate_random_persona_tool,\n",
    "    ]\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a persona-analysis agent. \"\n",
    "        \"ALWAYS use tools to: \"\n",
    "        \"1) load persona/bridging defs, 2) extract bridging, 3) create importance graph, 4) infer persona. \"\n",
    "        \"Return concise outputs between tool calls.\"\n",
    "    )\n",
    "\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=tools,\n",
    "        checkpointer=memory,\n",
    "        state_modifier=system_message,\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 타겟 LLM 생성\n",
    "# ============================================================================\n",
    "\n",
    "def create_target_llm(model_name: str, persona: Dict[str, Any], device: str = \"auto\"):\n",
    "    \"\"\"타겟 LLM 생성 - 특정 페르소나를 가진 대화 상대 (Hugging Face 모델)\"\"\"\n",
    "    persona_text = format_persona_for_prompt(persona)\n",
    "    system_message = f\"\"\"You are a person with the following persona:\n",
    "\n",
    "{persona_text}\n",
    "\n",
    "Engage in conversation fully immersed in this persona.\n",
    "When answering questions, naturally reflect the characteristics, background, and values of this persona.\n",
    "Don't list persona information directly; let it emerge naturally through conversation.\n",
    "Keep your responses concise and natural (2-4 sentences).\"\"\"\n",
    "\n",
    "    model = HuggingFaceModelWrapper(model_name, device=device)\n",
    "    return model, system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fd4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 메인 실행 함수\n",
    "# ============================================================================\n",
    "\n",
    "def run_interview_system(\n",
    "    openai_api_key: str,\n",
    "    target_model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    num_questions: int = 5,\n",
    "    device: str = \"auto\",\n",
    "):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Persona Interview System\")\n",
    "    print(f\"Tool Agent: OpenAI model\")\n",
    "    print(f\"Target LLM: {target_model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # 메모리/스레드 설정\n",
    "    memory = MemorySaver()\n",
    "    config = {\"configurable\": {\"thread_id\": \"interview-session-1\"}, \"recursion_limit\": 50}\n",
    "\n",
    "    # 타겟 페르소나 생성\n",
    "    target_persona = create_target_persona()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Generated Target Persona:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(json.dumps(target_persona, ensure_ascii=False, indent=2))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 에이전트 & 타겟 LLM\n",
    "    tool_agent = create_tool_agent(openai_api_key, memory)\n",
    "    target_model, target_system_msg = create_target_llm(target_model_name, target_persona, device)\n",
    "\n",
    "    conversation_history = []\n",
    "\n",
    "    # 초기 지시: 정의 로드 및 준비\n",
    "    initial_instruction = f\"\"\"Your mission:\n",
    "1. First, use tools to verify persona definition and bridging relationships.\n",
    "2. Then you will interview the target LLM through {num_questions} questions, one at a time.\n",
    "3. After the interview, you will analyze bridging relationships and create an importance graph.\n",
    "4. Finally, infer the target persona based on the collected information.\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Interview Started\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 초기 설정(툴 자동 실행)\n",
    "    tool_agent.invoke({\"messages\": [HumanMessage(content=initial_instruction)]}, config=config)\n",
    "\n",
    "    # 인터뷰 루프\n",
    "    for i in range(num_questions):\n",
    "        print(f\"\\n{'='*80}\\nQuestion {i+1}/{num_questions}\\n{'='*80}\")\n",
    "\n",
    "        # 질문 생성\n",
    "        question_prompt = f\"Now ask the target the {i+1}th question. Create a natural question that can help identify their persona.\"\n",
    "        result = tool_agent.invoke({\"messages\": [HumanMessage(content=question_prompt)]}, config=config)\n",
    "        agent_question = result[\"messages\"][-1].content\n",
    "        print(f\"[Tool Agent Question]: {agent_question}\")\n",
    "\n",
    "        # 타겟 LLM 답변\n",
    "        target_messages = [SystemMessage(content=target_system_msg), HumanMessage(content=agent_question)]\n",
    "        target_response = target_model.invoke(target_messages)\n",
    "        target_answer = target_response.content\n",
    "        print(f\"[Target Answer]: {target_answer}\")\n",
    "\n",
    "        conversation_history.append({\"question\": agent_question, \"answer\": target_answer})\n",
    "\n",
    "        # 다음 질문을 위한 피드백\n",
    "        feedback_prompt = (\n",
    "            f\"Target's answer: {target_answer}\\n\\n\"\n",
    "            f\"Analyze this answer in one short paragraph and prepare the next question.\"\n",
    "        )\n",
    "        tool_agent.invoke({\"messages\": [HumanMessage(content=feedback_prompt)]}, config=config)\n",
    "\n",
    "    # 최종 분석 단계(툴 사용 순서 지정)\n",
    "    print(f\"\\n{'='*80}\\nFinal Analysis\\n{'='*80}\")\n",
    "    conversation_text = json.dumps(conversation_history, ensure_ascii=False)\n",
    "\n",
    "    final_analysis_prompt = f\"\"\"\n",
    "The interview is complete. Use tools in this exact order:\n",
    "\n",
    "1) load_bridging_relationships to recall relation types.\n",
    "2) extract_bridging_from_conversation with the JSON conversation below.\n",
    "3) create_importance_graph using the JSON returned from step 2.\n",
    "4) Based on the graph, infer the target persona succinctly (3-5 sentences).\n",
    "\n",
    "Conversation (JSON):\n",
    "{conversation_text}\n",
    "\"\"\"\n",
    "    final_result = tool_agent.invoke({\"messages\": [HumanMessage(content=final_analysis_prompt)]}, config=config)\n",
    "    print(\"\\n[Agent Final Output]\\n\", final_result[\"messages\"][-1].content)\n",
    "\n",
    "    print(f\"\\n{'='*80}\\nInterview System Completed\\n{'='*80}\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results = {\n",
    "        \"target_model\": target_model_name,\n",
    "        \"target_persona\": target_persona,\n",
    "        \"conversation_history\": conversation_history,\n",
    "    }\n",
    "\n",
    "    output_filename = f\"interview_results_{target_model_name.replace('/', '_')}.json\"\n",
    "    save_json(results, output_filename)\n",
    "    print(f\"\\nResults saved to '{output_filename}'\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa31f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading environment variables from .env\n",
      "✅ Environment variables loaded successfully\n",
      "\n",
      "✅ OPENAI_API_KEY found\n",
      "✅ HF_TOKEN found\n",
      "\n",
      "================================================================================\n",
      "Persona Interview System\n",
      "Tool Agent: OpenAI model\n",
      "Target LLM: Qwen/Qwen2.5-0.5B-Instruct\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Generated Target Persona:\n",
      "================================================================================\n",
      "{\n",
      "  \"social_role\": \"student\",\n",
      "  \"personality\": {\n",
      "    \"openness\": \"1\",\n",
      "    \"conscientiousness\": \"1\",\n",
      "    \"extraversion\": \"0\",\n",
      "    \"agreeableness\": \"1\",\n",
      "    \"neuroticism\": \"0\"\n",
      "  },\n",
      "  \"background\": {\n",
      "    \"education\": \"unknown\",\n",
      "    \"culture\": \"unknown\"\n",
      "  },\n",
      "  \"interests\": {\n",
      "    \"hobby\": \"reading\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "❌ Error: create_agent() got an unexpected keyword argument 'state_modifier'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/2w/lkcqygdx72s_vktqq1wmb9p40000gn/T/ipykernel_13443/3644174785.py\", line 61, in <module>\n",
      "    _ = run_interview_system(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2w/lkcqygdx72s_vktqq1wmb9p40000gn/T/ipykernel_13443/2802049392.py\", line 30, in run_interview_system\n",
      "    tool_agent = create_tool_agent(openai_api_key, memory)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2w/lkcqygdx72s_vktqq1wmb9p40000gn/T/ipykernel_13443/2001530841.py\", line 385, in create_tool_agent\n",
      "    agent = create_agent(\n",
      "            ^^^^^^^^^^^^^\n",
      "TypeError: create_agent() got an unexpected keyword argument 'state_modifier'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 실행 스크립트\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def load_env_file(env_path=\".env\"):\n",
    "        \"\"\"Load environment variables from .env file\"\"\"\n",
    "        env_file = Path(env_path)\n",
    "        if env_file.exists():\n",
    "            print(f\"✅ Loading environment variables from {env_path}\")\n",
    "            with open(env_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "                        key, value = line.split(\"=\", 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip().strip('\"').strip(\"'\")\n",
    "                        if not os.environ.get(key):\n",
    "                            os.environ[key] = value\n",
    "            print(\"✅ Environment variables loaded successfully\\n\")\n",
    "        else:\n",
    "            print(f\"⚠️  No .env file found at {env_path}\")\n",
    "            print(\"You can create one with:\\n  OPENAI_API_KEY=your-key-here\\n  HF_TOKEN=your-token-here\\n\")\n",
    "\n",
    "    load_env_file()\n",
    "\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"❌ Error: OPENAI_API_KEY not found!\")\n",
    "        print(\"\\nPlease either:\")\n",
    "        print(\"  1. Create a .env file with: OPENAI_API_KEY=your-key\")\n",
    "        print(\"  2. Set environment variable: export OPENAI_API_KEY='your-key'\")\n",
    "        print(\"  3. Edit the code to set it directly\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    print(\"✅ OPENAI_API_KEY found\")\n",
    "    print(\"✅ HF_TOKEN found\" if HF_TOKEN else \"ℹ️  HF_TOKEN not set (needed only for gated models)\\n\")\n",
    "\n",
    "    # 모델 선택\n",
    "    # - \"Qwen/Qwen2.5-0.5B-Instruct\" (권장, 권한 불필요)\n",
    "    # - \"meta-llama/Llama-3.2-1B\" (권한 필요, HF_TOKEN 필요)\n",
    "    # - \"deepseek-ai/deepseek-vl2-tiny\" (Vision-Language 모델)\n",
    "    TARGET_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "    # Llama 모델 선택 시 HF 로그인 (선택)\n",
    "    if \"llama\" in TARGET_MODEL.lower():\n",
    "        if HF_TOKEN:\n",
    "            try:\n",
    "                from huggingface_hub import login\n",
    "                login(token=HF_TOKEN)\n",
    "                print(\"✅ Logged in to Hugging Face\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Hugging Face login failed: {e}\\n\")\n",
    "        else:\n",
    "            print(\"⚠️  Warning: Llama model may require HF_TOKEN!\\n\")\n",
    "\n",
    "    try:\n",
    "        _ = run_interview_system(\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            target_model_name=TARGET_MODEL,\n",
    "            num_questions=2,\n",
    "            device=\"auto\",  # \"cuda\", \"cpu\", \"mps\", or \"auto\"\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Interview interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b93fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
